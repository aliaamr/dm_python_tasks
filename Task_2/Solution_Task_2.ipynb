{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name : Alia Amr | Track : Data Management | Mail : aliaamr2110@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Descripition \n",
    "\n",
    "In 2012, URL shortening service Bitly partnered with the US government website USA.gov to provide a feed of anonymous data gathered from users who shorten links ending with .gov or .mil.\n",
    "\n",
    "The text file comes in JSON format and here are some keys and their description. They are only the most important ones for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'70f2663d9c2d23114ff9946c2d60a224'\n",
      "b'70f2663d9c2d23114ff9946c2d60a224'\n",
      "b'80db5c96f1fdfbec354dbd8d9d02656b'\n",
      "duplicated_files  1\n",
      "------------------------\n",
      "checksums  {'/home/aliaamr/Documents/Material/data/task_2/usa.gov_click_data_3.json': b'70f2663d9c2d23114ff9946c2d60a224', '/home/aliaamr/Documents/Material/data/task_2/usa.gov_click_data_2.json': b'70f2663d9c2d23114ff9946c2d60a224', '/home/aliaamr/Documents/Material/data/task_2/usa.gov_click_data_1.json': b'80db5c96f1fdfbec354dbd8d9d02656b'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from datetime import datetime\n",
    "from os.path import isfile, join\n",
    "from subprocess import PIPE, Popen\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# calcuate the execution time\n",
    "startTime = datetime.now() \n",
    "\n",
    "args = sys.argv\n",
    "args.pop(0)\n",
    "\n",
    "UNIX_format = False\n",
    "# check arguments\n",
    "if '-u' in args :\n",
    "    UNIX_format = True\n",
    "    args.remove('-u')\n",
    "\n",
    "directory_path = args[0]\n",
    "if(directory_path[0] != '/'):\n",
    "    directory_path = os.getcwd()+'/'+directory_path\n",
    "    \n",
    "checksums = {}\n",
    "duplicated_files = []\n",
    "\n",
    "file_list = [os.path.join(directory_path, f) for f in listdir(directory_path) if isfile(os.path.join(directory_path, f))]\n",
    "\n",
    "for file_name in file_list:\n",
    "    if fnmatch.fnmatch(file_name, '*.json'):\n",
    "        with Popen([\"md5sum\", file_name], stdout=PIPE) as proc:\n",
    "            checksum, _ = proc.stdout.read().split()\n",
    "            print(checksum)\n",
    "            if checksum in checksums.values():\n",
    "                duplicated_files.append(file_name)\n",
    "            checksums[file_name] = checksum\n",
    "            \n",
    "print(\"duplicated_files \" , len(duplicated_files) )\n",
    "print(\"------------------------\")\n",
    "print(\"checksums \" , (checksums) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json_file_fun (file_path):\n",
    "    try:\n",
    "        data_records = [json.loads(record) for record in open(file_path)]\n",
    "    except IOError:\n",
    "        print(\"[File Not Found.]\")\n",
    "    return data_records;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def filtered_data_records_fun (data_records):\n",
    "    # required columns\n",
    "    columns = ['a', 'c', 'r', 'u', 't', 'hc', 'cy', 'll']\n",
    "    filtered_data_records = []\n",
    "\n",
    "    for record in data_records: \n",
    "        record = {k:v for (k,v) in record.items() if k in columns}\n",
    "\n",
    "        # add missed columns with None as a default value\n",
    "        # then check None values and delet them\n",
    "        [record.setdefault(c, None) for c in columns]\n",
    "        if (None not in record.values()): \n",
    "            filtered_data_records.append(record)\n",
    "    return filtered_data_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retutn the url in short format\n",
    "def short_format_url(url):\n",
    "  return (url.replace('http://', '').replace('https://', '')).split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data_records_fun(filtered_data_records):\n",
    "\n",
    "    clear_data_records = []\n",
    "    for record in filtered_data_records: \n",
    "        # new empty record\n",
    "        new_data_record = {}\n",
    "\n",
    "        # browser and operating_system\n",
    "        splitted_record = record['a'].split(\" \")\n",
    "        # browser\n",
    "        new_data_record['web_browser'] = splitted_record[0]\n",
    "\n",
    "        # operating_system\n",
    "        if len(splitted_record) > 1:\n",
    "            # remove special characters like '(|,|;|)'\n",
    "            new_data_record['operating_system'] = \"\".join([character for character in splitted_record[1] if character.isalnum()])            \n",
    "        else:\n",
    "            continue # skip record if the operating_system is missed\n",
    "\n",
    "        # merge \n",
    "        new_data_record = {**new_data_record,**record}\n",
    "        del new_data_record['a']\n",
    "\n",
    "        # update url\n",
    "        new_data_record['r'] = short_format_url(record['u'])\n",
    "        new_data_record['u'] = short_format_url(record['r'])\n",
    "\n",
    "        # add longitude and latitude\n",
    "\n",
    "        new_data_record['long'] = new_data_record['ll'][0]\n",
    "        new_data_record['lat'] = new_data_record['ll'][1]\n",
    "        del new_data_record['ll']\n",
    "\n",
    "        # check UNIX_format option and update the dates  \n",
    "        if (UNIX_format == False):\n",
    "            if 't' in new_data_record:\n",
    "                ts = int(new_data_record['hc'])\n",
    "                new_data_record['hc'] = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                ts = int(new_data_record['t'])\n",
    "                new_data_record['t'] = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        clear_data_records.append(new_data_record)\n",
    "    return clear_data_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reordered_data_records_fun (clear_data_records):\n",
    "    # rearranging dictionary order\n",
    "    desired_order_list = ['web_browser', 'operating_system', 'r', 'u', 'cy', 'long', 'lat', 'c', 't', 'hc']\n",
    "    reordered_data_records = []\n",
    "\n",
    "    for record in clear_data_records:\n",
    "        reordered_data_records.append([record[key] for key in desired_order_list if key in record])\n",
    "    return reordered_data_records;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import errno\n",
    "from pathlib import Path    \n",
    "\n",
    "def save_file_fun (reordered_data_records, input_file_name):\n",
    "    input_file_name = Path(input_file_name).name\n",
    "    # remove the extension from the input file name\n",
    "    input_file_name = ''.join(input_file_name.rsplit('.',1)[:-1])\n",
    "\n",
    "    output_file_name = input_file_name + \"_output.csv\"\n",
    "    output_base_dir = directory_path+\"/transformed_files\"\n",
    "    output_file_path = os.path.join(output_base_dir, output_file_name)\n",
    "\n",
    "    # check and create the directory for the output\n",
    "    if not os.path.exists(os.path.dirname(output_file_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(output_file_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(output_file_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(reordered_data_records)\n",
    "    \n",
    "    print('-'*40)\n",
    "    print('File : ', output_file_name, ' generated successfully')\n",
    "    print('-'*40)\n",
    "    print('Output : ', len(reordered_data_records), 'row(s).')\n",
    "    print('_'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "File :  usa.gov_click_data_3_output.csv  generated successfully\n",
      "----------------------------------------\n",
      "Output :  2752 row(s).\n",
      "________________________________________\n",
      "Duplicated file :  /home/aliaamr/Documents/Material/data/task_2/usa.gov_click_data_2.json  checksum :  b'70f2663d9c2d23114ff9946c2d60a224'\n",
      "----------------------------------------\n",
      "File :  usa.gov_click_data_1_output.csv  generated successfully\n",
      "----------------------------------------\n",
      "Output :  8 row(s).\n",
      "________________________________________\n",
      "Task done successfully!\n",
      "Execution time :  0:00:00.613387\n"
     ]
    }
   ],
   "source": [
    "for file in file_list :\n",
    "    if file in duplicated_files:\n",
    "        print (\"Duplicated file : \" , file, \" checksum : \", checksums[file])\n",
    "    else:\n",
    "        r1 = open_json_file_fun (file)\n",
    "        r2 = filtered_data_records_fun (r1)\n",
    "        r3 = clear_data_records_fun (r2)\n",
    "        r4 = reordered_data_records_fun (r3)\n",
    "        r5 = save_file_fun (r4, file)\n",
    "print('Task done successfully!')\n",
    "print(\"Execution time : \", datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
